# To lock, or not to lock

TL;DR
=====

As I've promised you in my [previous post][0], I made [TLSnappy][9] balance and
handle requests a little bit better.

Data flow
=========

For leveraging all available CPUs TLSnappy runs multiple threads that are each
picking and processing tasks from their dispatch queues, one by one. Tasks are
created from node's event-loop in following cases:

* Data comes from client and should be decrypted
* Data from server should be encrypted

So, as you can see, each thread is receiving data from it's inputs (either
`encrypted` or `clear`) or/and emitting data to it's outputs. Following
pattern apparently requires a lot of data transfer `to` and `from` worker
threads and requires storing (buffering) that data in some temporary storage
before processing it.

To my mind, best structure to fit this needs is [Circular (Ring) buffer][5].
Because it's fast, can be grown if more than it's current capacity needs to be
held.

[Naive version][4] of it was good enough to try out things, but it wasn't
supposed to be run in multi-threaded concurrent environment - all access to this
buffer can take place only in a [critical sections][7]. This means that at any
time only one thread may access ring's methods or properties. One can suppose
that isn't much to care about, but, according to [Amdahl's law][8], reducing
time spent in non-parallelizable (sequential) parts of application is much more
critical for overall performance than speeding up parallel parts.

Lock-less ring buffer
=====================

Removing locks seemed to be essential for achieving better performance, however
special structure should to be constructed in order to make `Ring` work across
multiple CPUs. Here is the structure I chose for it:

![Ring buffer][3]

Ring consists of pages that're forming linked list (circle), each page has two
offsets: reader (`roffset`) and writer (`woffset`). And there're two special
pages (which could be the same one actually): reader head (`rhead`) and writer
head (`whead`).

So how does this work? Initially ring contains only one page which is `rhead`
and `whead` at the same time. When producer wants to put data in - it goes to
the `whead`, copies data into page, increments `woffset` and if page is full -
creates new page or reuses old one if it was fully read. Consumer takes `rhead`
reads up to `woffset - roffset` bytes from it, increments `roffset` and moves to
next page if `roffset` is equal to the size of page.

So here are benchmarks:

Without lock-less ring:
```
Transactions:                 200000 hits
Availability:                 100.00 %
Elapsed time:                  47.90 secs
Data transferred:             585.37 MB
Response time:                  0.02 secs
Transaction rate:            4175.37 trans/sec
Throughput:                    12.22 MB/sec
Concurrency:                   98.79
Successful transactions:      200000
Failed transactions:               0
Longest transaction:            0.09
Shortest transaction:           0.00
```

With lock-less ring:

```
Transactions:                 200000 hits
Availability:                 100.00 %
Elapsed time:                  47.37 secs
Data transferred:             585.37 MB
Response time:                  0.02 secs
Transaction rate:            4222.08 trans/sec
Throughput:                    12.36 MB/sec
Concurrency:                   98.83
Successful transactions:      200000
Failed transactions:               0
Longest transaction:            0.12
Shortest transaction:           0.00
```

As you can see, performance hasn't greatly improved and is actually almost
beyond statistical error (which means that results are nearly the same). However
these are results for small 3kb page, lets try sending some big 100kb buffers.

Without lock-less ring:
```
Transactions:                 100000 hits
Availability:                 100.00 %
Elapsed time:                  64.06 secs
Data transferred:            9536.74 MB
Response time:                  0.06 secs
Transaction rate:            1561.04 trans/sec
Throughput:                   148.87 MB/sec
Concurrency:                   98.59
Successful transactions:      100000
Failed transactions:               0
Longest transaction:            1.93
Shortest transaction:           0.00
```

With lock-less ring:
```
Transactions:                 100000 hits
Availability:                 100.00 %
Elapsed time:                  58.73 secs
Data transferred:            9536.74 MB
Response time:                  0.06 secs
Transaction rate:            1702.71 trans/sec
Throughput:                   162.38 MB/sec
Concurrency:                   98.98
Successful transactions:      100000
Failed transactions:               0
Longest transaction:            0.19
Shortest transaction:           0.00
```

Wow! That's much better - about 9% performance improvement.

Instruments
===========

Still TLSnappy's performance wasn't even close to what nginx is capable of
(~5100 requests per second). Thus it was necessary to continue investigation and
here goes on a stage [Instruments.app][10], which is basically an UI for some
very useful dtrace scripts. I've run `CPU Sampler` utility and this is what
call tree looked like:
![Original node][1]

Obviously it spends almost 30% of time in syncronization between threads,
particularly in `CRYPTO_add_lock` function:
![Old CRYPTO_add_lock][11]

After patching it this way (by using atomic operations, which are supported by
almost every CPU nowadays):
![New CRYPTO_add_lock][12]

Call tree locked like this:
![Patched node][2]

Results
=======

I've opened [pull request for node.js][13] and send the same patches to
openssl-dev mailing list. With patched node and latest tlsnappy this is a
benchmark results:

![Requests per second][14]
![Average load][15]

And that's without patches:

![Requests per second][16]
![Average load][17]

The work is unfinished yet, but now I know that OpenSSL doesn't really behave
well when used in multithreaded application.

[0]: http://blog.indutny.comtlsnappy/0.benchmarking-tls
[1]: https://raw.github.com/indutny/tlsnappy/master/benchmark/original-node.png
[2]: https://raw.github.com/indutny/tlsnappy/master/benchmark/patched-node.png
[3]: https://raw.github.com/indutny/tlsnappy/master/benchmark/ring.png
[4]: https://github.com/indutny/tlsnappy/blob/old-ring/src/ring.h
[5]: http://en.wikipedia.org/wiki/Circular_buffer
[6]: https://github.com/indutny/tlsnappy/blob/old-ring/src/tlsnappy.cc#L430-433
[7]: http://en.wikipedia.org/wiki/Critical_section
[8]: http://en.wikipedia.org/wiki/Amdahl's_law
[9]: https://github.com/indutny/tlsnappy
[10]: https://developer.apple.com/library/mac/#documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/Introduction/Introduction.html
[11]: https://raw.github.com/indutny/tlsnappy/master/benchmark/old-crypto-add-lock.png
[12]: https://raw.github.com/indutny/tlsnappy/master/benchmark/new-crypto-add-lock.png
[13]: https://github.com/joyent/node/pull/4105
[14]: https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps-2.png
[15]: https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-load-2.png
[16]: https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps-siege.png
[17]: https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-load-siege.png
